{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d8f1578-6f8c-444a-bfc6-666a141d7682",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yuzu as yz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd72a726-d937-447c-9bd3-6e736f50adde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yuzu import torch_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7475000d-54c7-4a34-91ce-dc3f722d2f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_graph.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99cb1e2b-f6a4-489d-b825-8657766eaffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from dataclasses import dataclass, field\n",
    "from abc import ABC, abstractmethod\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import gym\n",
    "from typing import Any, NamedTuple\n",
    "from yuzu import rllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8496812-2a29-4bfa-b0de-71b6fd15fa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "envname = 'Taxi-v3'\n",
    "class ClipRewardEnv(gym.RewardWrapper):\n",
    "    \"\"\"\n",
    "    Clips the reward to {+1, 0, -1} by its sign.\n",
    "    Args:\n",
    "        env (gym.Env): The environment to wrap\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env: gym.Env):\n",
    "        gym.RewardWrapper.__init__(self, env)\n",
    "    \n",
    "    def reward(self, reward: float) -> float:\n",
    "        return reward/100\n",
    "\n",
    "def create_env(mode='rgb_array'):\n",
    "    env = gym.make(envname, render_mode='rgb_array')\n",
    "    env = rllib.OneHotObservationWrapper(env)\n",
    "    env = rllib.YuzuObservationWrapper(env)\n",
    "    env = ClipRewardEnv(env)\n",
    "    return env\n",
    "env = create_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b5d6d1a-db6e-4988-87b8-5160b761a698",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 64\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, obs_size, act_size):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size,N),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(N,N),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(N,act_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.inner()\n",
    "        if x.ndim == 1:\n",
    "            x = torch.reshape(x, (-1,) + x.shape)\n",
    "        return yz.wrap(self.net(x))\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    return NeuralNetwork(env.observation_space.shape[0], env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "473e4f42-6250-4ee5-88e0-55fd5a94e83d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sunho/miniconda3/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 51\u001b[0m\n\u001b[1;32m     48\u001b[0m     score \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(stats\u001b[38;5;241m.\u001b[39mreward_history[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m128\u001b[39m:])\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m: score, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magent\u001b[39m\u001b[38;5;124m\"\u001b[39m: agent}\n\u001b[0;32m---> 51\u001b[0m agent \u001b[38;5;241m=\u001b[39m \u001b[43mobjective\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgamma\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreplay_buf_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mN\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain_count\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mupdate_interval\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m700000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m}\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "Cell \u001b[0;32mIn[7], line 47\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     45\u001b[0m options\u001b[38;5;241m.\u001b[39mreport_reward \u001b[38;5;241m=\u001b[39m add_reward\n\u001b[1;32m     46\u001b[0m options\u001b[38;5;241m.\u001b[39mreport_train \u001b[38;5;241m=\u001b[39m add_loss\n\u001b[0;32m---> 47\u001b[0m stats \u001b[38;5;241m=\u001b[39m \u001b[43mrllib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdqn_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcreate_env\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m score \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(stats\u001b[38;5;241m.\u001b[39mreward_history[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m128\u001b[39m:])\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m: score, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magent\u001b[39m\u001b[38;5;124m\"\u001b[39m: agent}\n",
      "File \u001b[0;32m~/dev/KatoML/src/yuzu/yuzu/rllib.py:325\u001b[0m, in \u001b[0;36mdqn_train\u001b[0;34m(create_env, agent, device, options)\u001b[0m\n\u001b[1;32m    322\u001b[0m     y \u001b[38;5;241m=\u001b[39m b_rewards\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1.0\u001b[39m\u001b[38;5;241m-\u001b[39mb_dones)\u001b[38;5;241m*\u001b[39moptions\u001b[38;5;241m.\u001b[39mgamma\u001b[38;5;241m*\u001b[39mqvals_target\u001b[38;5;241m.\u001b[39mreduce_max(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    323\u001b[0m loss \u001b[38;5;241m=\u001b[39m yz\u001b[38;5;241m.\u001b[39mwrap(F\u001b[38;5;241m.\u001b[39msmooth_l1_loss(qvals\u001b[38;5;241m.\u001b[39minner(), y\u001b[38;5;241m.\u001b[39minner()))\n\u001b[0;32m--> 325\u001b[0m \u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    326\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    327\u001b[0m options\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/_compile.py:24\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dynamo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:489\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m     dynamo_config_ctx\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__enter__\u001b[39m()\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 489\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    491\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/optim/optimizer.py:815\u001b[0m, in \u001b[0;36mOptimizer.zero_grad\u001b[0;34m(self, set_to_none)\u001b[0m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    813\u001b[0m     per_device_and_dtype_grads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 815\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_zero_grad_profile_name):\n\u001b[1;32m    816\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups:\n\u001b[1;32m    817\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/autograd/profiler.py:605\u001b[0m, in \u001b[0;36mrecord_function.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 605\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecord \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_record_function_enter_new\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/_ops.py:755\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    750\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    751\u001b[0m     \u001b[38;5;66;03m# overloading __call__ to ensure torch.ops.foo.bar()\u001b[39;00m\n\u001b[1;32m    752\u001b[0m     \u001b[38;5;66;03m# is still callable from JIT\u001b[39;00m\n\u001b[1;32m    753\u001b[0m     \u001b[38;5;66;03m# We save the function ptr as the `op` attribute on\u001b[39;00m\n\u001b[1;32m    754\u001b[0m     \u001b[38;5;66;03m# OpOverloadPacket to access it here.\u001b[39;00m\n\u001b[0;32m--> 755\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "ri = 0\n",
    "li = 0\n",
    "def objective(config):\n",
    "    global ri\n",
    "    global li\n",
    "    ri = 0\n",
    "    li = 0\n",
    "    suffix = \"lr={},gamma={},replay_buf_size={},batch_size={},train_count={},update_interval={},eps_decay={},N={}\".format(\n",
    "        config[\"lr\"], config[\"gamma\"], config[\"replay_buf_size\"], config[\"batch_size\"], config[\"train_count\"], \n",
    "        config[\"update_interval\"], config[\"eps_decay\"], config['N'])\n",
    "    # writer = SummaryWriter(logdir+suffix)\n",
    "    writer = SummaryWriter()\n",
    "    \n",
    "    def create_model():\n",
    "        return NeuralNetwork(env.observation_space.shape[0], env.action_space.n)\n",
    "    model = create_model()\n",
    "    #300000\n",
    "    agent = rllib.DQNAgent(model, create_model(), None, env.action_space, 1.0, 0.0, config[\"eps_decay\"])\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "    options = rllib.DQNOptions(optimizer)\n",
    "    options.replay_buf_size = config[\"replay_buf_size\"]\n",
    "    options.num_steps = 1024000\n",
    "    options.double_dqn = True\n",
    "    options.max_episode_len = 128\n",
    "    options.gamma = config[\"gamma\"]\n",
    "    options.update_interval = config[\"update_interval\"]\n",
    "    options.batch_size = config[\"batch_size\"]\n",
    "    options.train_count =  config[\"train_count\"]\n",
    "    options.train_interval = 128\n",
    "    \n",
    "    def add_reward(x):\n",
    "        global ri\n",
    "        writer.add_scalar(\"Reward/train\", x, ri)\n",
    "        ri += 1\n",
    "    \n",
    "    def add_loss(x):\n",
    "        global li\n",
    "        for k,v in x.items():\n",
    "            writer.add_scalar(\"Loss/\" + k, v, li)\n",
    "        li += 1\n",
    "    \n",
    "    options.report_reward = add_reward\n",
    "    options.report_train = add_loss\n",
    "    stats = rllib.dqn_train(create_env, agent, None, options)\n",
    "    score = np.mean(stats.reward_history[-128:])\n",
    "    return {\"score\": score, \"model\": model, \"agent\": agent}\n",
    "\n",
    "agent = objective({\n",
    "    'lr': 0.001,\n",
    "    'gamma': 0.9,\n",
    "    'replay_buf_size': 256*128,\n",
    "    'batch_size': 128,\n",
    "    'N': 64,\n",
    "    'train_count': 1,\n",
    "    'update_interval': 1000,\n",
    "    'eps_decay': 700000,\n",
    "})[\"agent\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a84683c-1134-47bc-9f22-4adc4d03c90c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767c91c3-79cc-4c66-952a-cdb55425f097",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
